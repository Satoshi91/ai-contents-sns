import { streamText, convertToCoreMessages } from 'ai';
import { createOpenAI } from '@ai-sdk/openai';

// OpenRouterç”¨ã®OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®šï¼ˆãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½ä»˜ãï¼‰
const openrouter = createOpenAI({
  apiKey: process.env.OPENROUTER_API_KEY,
  baseURL: 'https://openrouter.ai/api/v1',
  compatibility: 'compatible', // OpenRouterã¨ã®äº’æ›æ€§ã‚’æ˜ç¤º
  fetch: async (url, options) => {
    console.log('ğŸŒ [OpenRouter] Request:', {
      url: url.toString(),
      method: options?.method || 'GET',
      headers: options?.headers ? Object.fromEntries(
        Object.entries(options.headers).map(([k, v]) => 
          k.toLowerCase() === 'authorization' ? [k, `Bearer ${(v as string).substring(7, 20)}...`] : [k, v]
        )
      ) : {},
      body: options?.body ? JSON.parse(options.body as string) : null
    });
    
    const response = await fetch(url, options);
    
    console.log('ğŸŒ [OpenRouter] Response:', {
      status: response.status,
      statusText: response.statusText,
      headers: Object.fromEntries(response.headers.entries()),
    });
    
    // ãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹ã‚’ç¢ºèª
    if (!response.ok) {
      const errorText = await response.clone().text();
      console.log('ğŸŒ [OpenRouter] Error response body:', errorText);
    }
    
    return response;
  }
});

const SYSTEM_PROMPT = `
ã‚ãªãŸã¯VOICARISMEã®AIãƒãƒ£ãƒƒãƒˆã§ã™ã€‚
AIãƒœã‚¤ã‚¹ãƒ‰ãƒ©ãƒã¨ASMRä½œå“ã®å‰µä½œã‚’æ”¯æ´ã™ã‚‹å°‚é–€å®¶ã¨ã—ã¦ã€
ä»¥ä¸‹ã®åˆ†é‡ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ãã ã•ã„ï¼š

1. ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šã¨å°æœ¬ä½œæˆ
2. SSMLå½¢å¼ã§ã®éŸ³å£°æ¼”å‡ºæœ€é©åŒ–
3. æ„Ÿæƒ…è¡¨ç¾ã¨è©±é€Ÿèª¿æ•´ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹
4. ASMRã‚·ãƒŠãƒªã‚ªã®æ§‹æˆææ¡ˆ
5. å‰µä½œæ´»å‹•å…¨èˆ¬ã®ã‚µãƒãƒ¼ãƒˆ

å¸¸ã«å‰µé€ çš„ã§å®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’å¿ƒãŒã‘ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å‰µä½œæ„å›³ã‚’ç†è§£ã—ã¦
å…·ä½“çš„ã§å½¹ç«‹ã¤ææ¡ˆã‚’è¡Œã£ã¦ãã ã•ã„ã€‚
`;

export async function POST(req: Request) {
  // å¼·åˆ¶çš„ã«ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
  const log = (message: string) => {
    console.log(message);
    console.error(message); // ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã«ã‚‚å‡ºåŠ›ã—ã¦ç¢ºå®Ÿã«è¡¨ç¤º
  };
  
  try {
    log('ğŸš€ [API] ===== CHAT REQUEST START =====');
    const { messages } = await req.json();
    log('ğŸ“¨ [API] Messages received: ' + JSON.stringify(messages, null, 2));

    const coreMessages = convertToCoreMessages(messages);
    log('ğŸ”„ [API] Converted to core messages: ' + JSON.stringify(coreMessages, null, 2));

    const modelName = process.env.NEXT_PUBLIC_DEFAULT_AI_MODEL || 'openai/gpt-4o-mini';
    const hasApiKey = !!process.env.OPENROUTER_API_KEY;
    const apiKeyPrefix = process.env.OPENROUTER_API_KEY?.substring(0, 10) || 'none';
    
    log('ğŸ”§ [API] Configuration:');
    log('  Model: ' + modelName);
    log('  API Key present: ' + hasApiKey);
    log('  API Key prefix: ' + apiKeyPrefix + '...');
    log('  OpenRouter URL: https://openrouter.ai/api/v1');
    
    log('ğŸ¤– [API] Calling streamText...');
    const result = await streamText({
      model: openrouter(process.env.NEXT_PUBLIC_DEFAULT_AI_MODEL || 'openai/gpt-4o-mini'),
      system: SYSTEM_PROMPT,
      messages: coreMessages,
      temperature: 0.7,
      maxRetries: 3,
    });

    log('âœ… [API] streamText completed successfully');
    log('ğŸ“Š [API] Result object properties: ' + Object.keys(result).join(', '));
    
    return result.toTextStreamResponse();
  } catch (error) {
    const errorMessage = error instanceof Error ? error.message : 'Unknown error';
    const errorStack = error instanceof Error ? error.stack : 'No stack trace';
    
    log('âŒ [API] ===== CHAT REQUEST ERROR =====');
    log('Error message: ' + errorMessage);
    log('Error stack: ' + errorStack);
    log('=====================================');
    
    return new Response(
      JSON.stringify({ 
        error: 'Internal server error',
        details: errorMessage,
        timestamp: new Date().toISOString()
      }),
      { 
        status: 500,
        headers: { 'Content-Type': 'application/json' }
      }
    );
  }
}